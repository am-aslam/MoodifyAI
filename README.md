# AI Flappy Bird â€” DQN Agent (PyTorch)

This repo implements a **Deep Q-Network (DQN)** agent that learns to play a simplified **Flappy Bird** built with `pygame`.
It uses feature-based state (no raw pixels) for fast training.

## ğŸ”§ Tech
- Python 3.10+
- PyTorch
- pygame
- numpy
- matplotlib (for optional reward plots)

## ğŸ“¦ Install
```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

pip install -r requirements.txt
```

## â–¶ï¸ Train
```bash
python -m flappy.train
```
- Checkpoints are saved to `models/`.
- Training logs are written to `runs/`.
- Use `CTRL+C` to stop; the latest checkpoint is preserved.

### Key hyperparameters
Edit `flappy/config.py`:
- `EPISODES`, `MAX_STEPS`
- `LR`, `GAMMA`, `BATCH_SIZE`, `EPS_START`, `EPS_END`, `EPS_DECAY`
- `TARGET_UPDATE`

## ğŸ® Watch the Trained Agent
```bash
python -m flappy.play --model models/best.pt
```
You can also specify a different checkpoint:
```bash
python -m flappy.play --model models/ckpt_last.pt
```

## ğŸ§  State Representation
State vector (normalized):
1. Bird y position
2. Bird vertical velocity
3. Horizontal distance to next pipe
4. Next pipe top y
5. Next pipe bottom y

## ğŸ† Rewards
- +1 per step survived
- +100 for passing a pipe
- -100 on collision
- -0.05 per flap (to discourage spamming)

## ğŸ—‚ï¸ Structure
```
ai-flappy-dqn/
â”œâ”€â”€ flappy/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ dqn.py
â”‚   â”œâ”€â”€ game.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ play.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ models/
â”œâ”€â”€ runs/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## â— Notes
- Training speed depends on CPU/GPU. Pixel-based RL is intentionally avoided for practicality.
- You can tweak physics or visuals in `game.py` (gravity, flap impulse, pipe gap, spawn rate).
- This is a learning-focused implementation; feel free to optimize further.
